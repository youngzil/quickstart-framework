CPU缓存分类：CPU三级缓存：CPU、L1、L2、L3、主内存
缓存一致性协议：MESI
伪共享(False Sharing)：缓存行进行字节填充，保证头尾加载到不同的缓存行，避免出入对互斥，不能并发，成为伪共享。jdk8中Contended注解
缓存系统中是以缓存行（cache line）为单位存储的：现在64byte，早期32byte，范围32-128byte
缓存段竞争：导致总线风暴
volatile字段：使用内存屏障，总线嗅探技术，保证可见性和有序性，不保证原子性




CPU是你机器的心脏，最终由它来执行所有运算和程序。主内存（RAM）是你的数据（包括代码行）存放的地方。

CPU和主内存之间有好几层缓存，因为即使直接访问主内存也是非常慢的。
如果你正在多次对一块数据做相同的运算，那么在执行运算的时候把它加载到离CPU很近的地方就有意义了（比如一个循环计数－你不想每次循环都跑到主内存去取这个数据来增长它吧）。


CPU三级缓存：CPU、L1、L2、L3、主内存

越靠近CPU的缓存越快也越小。所以L1缓存很小但很快(译注：L1表示一级缓存)，并且紧靠着在使用它的CPU内核。L2大一些，也慢一些，并且仍然只能被一个单独的 CPU 核使用。L3在现代多核机器中更普遍，仍然更大，更慢，并且被单个插槽上的所有 CPU 核共享。最后，你拥有一块主存，由全部插槽上的所有 CPU 核共享。


当CPU执行运算的时候，它先去L1查找所需的数据，再去L2，然后是L3，最后如果这些缓存中都没有，所需的数据就要去主内存拿。走得越远，运算耗费的时间就越长。所以如果你在做一些很频繁的事，你要确保数据在L1缓存中。


数据在缓存中不是以独立的项来存储的，如不是一个单独的变量，也不是一个单独的指针。
缓存是由缓存行组成的，通常是64字节（译注：这篇文章发表时常用处理器的缓存行是64字节的，比较旧的处理器缓存行是32字节），并且它有效地引用主内存中的一块地址。一个Java的long类型是8字节，因此在一个缓存行中可以存8个long类型的变量。

非常奇妙的是如果你访问一个long数组，当数组中的一个值被加载到缓存中，它会额外加载另外7个。因此你能非常快地遍历这个数组。事实上，你可以非常快速的遍历在连续的内存块中分配的任意数据结构。

数组内元素的内存地址的连续性存储的，只要一个元素被加载到缓存行，其他相邻的几个元素也会被加载进同一个缓存行

因此如果你数据结构中的项在内存中不是彼此相邻的（链表，我正在关注你呢），你将得不到免费缓存加载所带来的优势。并且在这些数据结构中的每一个项都可能会出现缓存未命中。


我们必须以整个缓存行作为单位来处理（译注：这是CPU的实现所规定的，详细可参见深入分析Volatile的实现原理）


伪共享(False Sharing)
“伪共享”（译注：可以理解为错误的共享）
这三个变量常常在一个cache line里面，有可能导致false sharing。
如把head和tail加载到一个缓存行，将出现伪共享，不能并发


缓存系统中是以缓存行（cache line）为单位存储的。
缓存行是2的整数幂个连续字节，一般为32-256个字节。最常见的缓存行大小是64个字节。
当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。缓存行上的写竞争是运行在SMP系统中并行线程实现可伸缩性最重要的限制因素。有人将伪共享描述成无声的性能杀手，因为从代码中很难看清楚是否会出现伪共享。



Disruptor消除这个问题，至少对于缓存行大小是64字节或更少的处理器架构来说是这样的（译注：有可能处理器的缓存行是128字节，那么使用64字节填充还是会存在伪共享问题），通过增加补全来确保ring buffer的序列号不会和其他东西同时存在于一个缓存行中。

因此没有伪共享，就没有和其它任何变量的意外冲突，没有不必要的缓存未命中。


在你的Entry类中也值得这样做，如果你有不同的消费者往不同的字段写入，你需要确保各个字段间不会出现伪共享。


内存屏障
编译器和CPU可以在保证输出结果一样的情况下对指令重排序，使性能得到优化。插入一个内存屏障，相当于告诉CPU和编译器先于这个命令的必须先执行，后于这个命令的必须后执行。
内存屏障另一个作用是强制更新一次不同CPU的缓存。例如，一个写屏障会把这个屏障前写入的数据刷新到缓存，这样任何试图读取该数据的线程将得到最新值，而不用考虑到底是被哪个cpu核心或者哪颗CPU执行的。


这里有个神奇咒语叫volatile(我觉得这个词在Java规范中从未被解释清楚)。如果你的字段是volatile，Java内存模型将在写操作后插入一个写屏障指令，在读操作前插入一个读屏障指令。

这意味着如果你对一个volatile字段进行写操作，你必须知道：
1、一旦你完成写入，任何访问这个字段的线程将会得到最新的值。
2、在你写入前，会保证所有之前发生的事已经发生，并且任何更新过的数据值也是可见的，因为内存屏障会把之前的写入值都刷新到缓存。


对性能的影响

内存屏障作为另一个CPU级的指令，没有锁那样大的开销。内核并没有在多个线程间干涉和调度。但凡事都是有代价的。内存屏障的确是有开销的——编译器/cpu不能重排序指令，导致不可以尽可能地高效利用CPU，另外刷新缓存亦会有开销。所以不要以为用volatile代替锁操作就一点事都没。




http://ifeve.com/from-javaeye-false-sharing/
http://ifeve.com/from-javaeye-cpu-cache/



volatile原理：http://blog.csdn.net/opensure/article/details/46669337
Java中Volatile底层原理与应用：内存屏障
http://blog.csdn.net/u012465296/article/details/53020676
volatile是一种“轻量级的锁”，它能保证锁的可见性，但不能保证锁的原子性。 

http://ifeve.com/memory-barriers-or-fences/
内存屏障或内存栅栏，也就是让一个CPU处理单元中的内存状态对其它处理单元可见的一项技术。
Java内存模型中volatile变量在写操作之后会插入一个store屏障，在读操作之前会插入一个load屏障。

缓存一致性：
http://www.infoq.com/cn/articles/cache-coherency-primer
http://www.infoq.com/cn/articles/atomic-operations-and-contention
MESI－CPU缓存一致性协议：http://blog.csdn.net/realxie/article/details/7317630
volatile使用场景：Double-Check Locking，缓存对象的map等（不保证原子性，使用在并发环境下的原子操作情况下）
一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义：
1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。
2）禁止进行指令重排序。
3）并发编程中的三个概念：原子性，可见性，有序性，volatile只保证可见性和有序性，不保证原子性
1、CPU的三级缓存，volatile的缓存失效是指的cpu的二级缓存，CPU的三级缓存：一级在CPU内部，容量小，速度快，二级是在CPU和内存之间的，当线程需要读取数据时候，先读取缓存，不存在才读取内存数据，三级缓存是对二级缓存的补充。
2、普通变量与volatile变量的区别是volatile的特殊规则保证了新值能立即同步到主内存，以及每次使用前可以立即从内存刷新，即一个线程修改了某个变量的值，其它线程读取的话肯定能看到新的值；
3、Lock前缀指令会引起处理器缓存回写到内存。一个处理器的缓存回写到内存会导致其他处理器的缓存无效。
1）将当前处理器缓存行的数据会写回到系统内存。
它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为“缓存锁定”，缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据。
2）这个写回内存的操作会引起在其他CPU里缓存了该内存地址的数据无效。
关键点：其实相当于线程 像缓存行写数据的时候，会锁住缓存行，是其他线程不能读，写完后失效缓存行，其他线程便可以从内存读到共享变量的最新值了；
4、如果对声明了Volatile变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题，所以在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据进行修改操作的时候，会强制重新从系统内存里把数据读到处理器缓存里。

当CPU看到一条读内存的指令时，它会把内存地址传递给一级数据缓存，一级数据缓存会检查它是否有这个内存地址对应的缓存段。如果没有，它会把整个缓存段从内存（或者从更高一级的缓存，如果有的话）中加载进来。是的，一次加载整个缓存段，这是基于这样一个假设：内存访问倾向于本地化（localized），如果我们当前需要某个地址的数据，那么很可能我们马上要访问它的邻近地址。一旦缓存段被加载到缓存中，读指令就可以正常进行读取。

如果我们只处理读操作，那么事情会很简单，因为所有级别的缓存都遵守以下规律，我称之为：
基本定律：在任意时刻，任意级别缓存中的缓存段的内容，等同于它对应的内存中的内容。
一旦我们允许写操作，事情就变得复杂一点了。这里有两种基本的写模式：直写（write-through）和回写（write-back）。

直接模式更简单，但是回写模式有它的优势：它能过滤掉对同一地址的反复写操作，并且，如果大多数缓存段都在回写模式下工作，那么系统经常可以一下子写一大片内存，而不是分成小块来写，前者的效率更高。

不同的级别缓存可能使用不同的写策略，所以缓存行大小也可能不同。

MESI定律：在所有的脏缓存段（M状态）被回写后，任意缓存级别的所有缓存段中的内容，和它们对应的内存中的内容一致。此外，在任意时刻，当某个位置的内存被一个处理器加载入独占缓存段时（E状态），那它就不会再出现在其他任何处理器的缓存中。
MESI协议（译者注：MESI是Modified、Exclusive、Shared、Invalid的首字母缩写，代表四种缓存状态
失效（Invalid）缓存段、共享（Shared）缓存段、独占（Exclusive）缓存段、已修改（Modified）缓存段，属于脏段
只有E、M状态下的缓存才能写，也就是在写之前，先申请独占，使其他的CPU的同段缓存失效

1、缓存行：所谓缓存行就是缓存中可以分配的最小存储单位。一般是64字节，老的是32字节如P6系列和奔腾处理器，它们的L1和L2高速缓存行是32个字节宽
缓存是分“段”（line）的，一个段对应一块存储空间，大小是32（较早的ARM、90年代/2000年代早期的x86和PowerPC）、64（较新的ARM和x86）或128（较新的Power ISA机器）字节。
我们会看到有CPU的一级缓存是32字节，而二级缓存却有128字节。
2、缓存伪共享：因为对于英特尔酷睿i7，酷睿， Atom和NetBurst， Core Solo和Pentium M处理器的L1，L2或L3缓存的高速缓存行是64个字节宽，不支持部分填充缓存行，导致一个缓存行加载多个共享变量，一个处理器锁定时，锁定该缓存行时候，多锁定了对象，导致其他处理器处理不了，变为锁的互斥性，导致不能并发，成为伪共享
如JDK7的并发包里新增一个队列集合类LinkedTransferQueue（它使用一个内部类类型来定义队列的头队列（Head）和尾节点（tail）），JDK加载时候会把头尾分别进行字节填充，保证头尾加载到不同的缓存行，避免出入对互斥，不能并发，成为伪共享。

Java8中用sun.misc.Contended避免伪共享(false sharing)：
https://blog.csdn.net/aigoogle/article/details/41518369
1、long padding来避免伪共享 
2、jdk8新特性，Contended注解避免false sharing：需要在jvm启动时设置-XX:-RestrictContended 
3、CPU处理器的缓存三级：L1、L2、L3，最早CPU和L1通信，L1和内存通信，现在CPU--》L1--》L2--》内存

Contended注解在进行缓存行填充的时候是按缓存行大小的2倍在被填充的内存数据的前后分别进行填充的。

L1　Cache(一级缓存)是CPU第一层高速缓存，分为数据缓存和指令缓存。内置的L1高速缓存的容量和结构对CPU的性能影响较大，不过高速缓冲存储器均由静态RAM组成，结构较复杂，容量小
L2 Cache 都在CPU中
L3　Cache(三级缓存)，分为两种，早期的是外置，现在的都是内置的。而它的实际作用即是，L3缓存的应用可以进一步降低内存延迟，同时提升大数据量计算时处理器的性能。降低内存延迟和提升大数据量计算能力对游戏都很有帮助。

4、缓存一致性协议
MESI定律：在所有的脏缓存段（M状态）被回写后，任意缓存级别的所有缓存段中的内容，和它们对应的内存中的内容一致。此外，在任意时刻，当某个位置的内存被一个处理器加载入独占缓存段时（E状态），那它就不会再出现在其他任何处理器的缓存中。
MESI协议（译者注：MESI是Modified、Exclusive、Shared、Invalid的首字母缩写，代表四种缓存状态
失效（Invalid）缓存段、共享（Shared）缓存段、独占（Exclusive）缓存段、已修改（Modified）缓存段，属于脏段
只有E、M状态下的缓存才能写，也就是在写之前，先申请独占，使其他的CPU的同段缓存失效

缓存段竞争：要产生缓存段竞争，我们需要多个处理器频繁访问同一缓存段，并且其中部分的访问是写操作。
如果两个或多个处理器频繁地访问相同的缓存段，那么这些缓存段的内容必须保持同步。如果想更新其中一个缓存段的内容，必须先获得独占权，这意味着其他所有处理器必须先丢弃它们缓存中的同一缓存段的拷贝。这带来的结果是，下一次有另外一个处理器要访问这个缓存段，它的内容必须先通过总线来加载。所以结果就是缓存失效率（对于其他处理器来说）和总线上额外的通讯流量都增加了。这种多个处理器访问一个频繁被更新的缓存段的现象，叫做“缓存（段）竞争”。如果你想在多个处理器共用内存的环境中拖慢一个并行的程序，这也许是最简单的方法。
总线风暴：总线上额外的通讯流量增加很多


